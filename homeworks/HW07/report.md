# HW07 – Report

> Файл: `homeworks/HW07/report.md`  
> Важно: не меняйте названия разделов (заголовков). Заполняйте текстом и/или вставляйте результаты.

## 1. Datasets

Выбраны 3 датасета из 4: DS1, DS2, DS4.

### 1.1 Dataset A

- Файл: `hw07_ds1.csv`  
- Размер: 12 000 строк × 9 столбцов  
- Признаки: все числовые (float64), плюс `sample_id` (int64)  
- Пропуски: отсутствуют  
- "Подлости" датасета: признаки сильно различаются по шкале и распределению (от -215 до +213 для некоторых фич), есть выбросы, высокая дисперсия, все числовые  

### 1.2 Dataset B

- Файл: `hw07_ds2.csv`  
- Размер: 8 000 строк × 4 столбца  
- Признаки: числовые (float64) + `sample_id` (int64)  
- Пропуски: отсутствуют  
- "Подлости" датасета: шумовой признак `z_noise` с широким диапазоном значений (до ±34), распределение данных различное по признакам, возможны выбросы, низкая размерность  

### 1.3 Dataset C

- Файл: `hw07_ds4.csv`  
- Размер: 10 000 строк × 33 столбца  
- Признаки: числовые (n01–n30) + категориальные (`cat_a`, `cat_b`) + `sample_id`  
- Пропуски: есть в числовых признаках, до ~224 пропусков на столбец  
- "Подлости" датасета: смешанные типы данных, пропуски, высокая размерность, различные шкалы числовых признаков, категориальные признаки с разными cardinality  

## 2. Protocol

- **Препроцессинг**:  
  - DS1 и DS2 – только числовые признаки, стандартизация (StandardScaler)  
  - DS4 – числовые признаки: имputation (`median`) + масштабирование, категориальные: imputation (`most_frequent`) + OneHotEncoder (`sparse_output=False`)  
- **Поиск гиперпараметров**:  
  - KMeans: `k` от 2 до 10, `n_init=10`, фиксированный `random_state`  
  - DBSCAN: `eps` линейно от 0.1 до 3, `min_samples=5`  
  - Agglomerative: `k` от 2 до 10, linkage = ['ward', 'average']  
  - Выбор лучшего метода и параметров через максимальный silhouette score  
- **Метрики**: silhouette, Davies-Bouldin, Calinski-Harabasz. Для DBSCAN: вычисление метрик только на кластерах (исключая шум), доля шума учитывалась отдельно  
- **Визуализация**: PCA 2D для всех датасетов (`n_components=2`, `random_state=42`), scatter plot с цветом по кластерам  

## 3. Models

- **DS1**: KMeans (подбор `k`), фиксированный `random_state`, `n_init=10`  
- **DS2**: DBSCAN (`eps`/`min_samples`), доля шума  
- **DS4**: AgglomerativeClustering (`k`, `linkage`)  
- Дополнительно: проверка устойчивости KMeans на DS1 (5 запусков с разными seed)  

## 4. Results

### 4.1 Dataset A (DS1)

- Лучший метод и параметры: KMeans, `n_clusters=4`  
- Метрики: silhouette=0.3833, Davies-Bouldin=1.1603, Calinski-Harabasz=9427.50  
- Доля шума: отсутствует  
- Коротко: KMeans стабильно делит числовой датасет с нормализованными признаками, результат устойчивый (ARI=1.0 между всеми запусками)  

### 4.2 Dataset B (DS2)

- Лучший метод и параметры: DBSCAN, `eps=0.4053`, `min_samples=5`  
- Метрики: silhouette=0.1362, Davies-Bouldin=0.6077, Calinski-Harabasz=32.78  
- Доля шума: 6.16%  
- Коротко: шумной датасет с небольшой плотностью, DBSCAN лучше выявляет плотные кластеры, хотя silhouette низкий  

### 4.3 Dataset C (DS4)

- Лучший метод и параметры: AgglomerativeClustering, linkage=ward, `n_clusters=5`  
- Метрики: silhouette=0.4474, Davies-Bouldin=0.9759, Calinski-Harabasz=5087.69  
- Доля шума: отсутствует  
- Коротко: смешанный датасет с пропусками и категориальными признаками, иерархическая кластеризация позволяет учесть сложную структуру данных  

## 5. Analysis

### 5.1 Сравнение алгоритмов (важные наблюдения)

- KMeans "ломается", когда кластеры имеют разные плотности или шум (DS2)  
- DBSCAN выигрывает на датасетах с шумовыми точками и неравномерной плотностью  
- AgglomerativeClustering лучше на смешанных типах признаков и при пропусках после имputation + one-hot кодирования  
- На результат сильнее всего влияют: масштабирование признаков, наличие выбросов, пропуски и смешанные типы признаков  

### 5.2 Устойчивость (DS1)

- Проверка: KMeans 5 запусков с разными seed  
- Результат: ARI=1.0 между всеми запусками  
- Вывод: KMeans на DS1 полностью устойчив, т.к. данные числовые и хорошо масштабированы  

### 5.3 Интерпретация кластеров

- DS1: кластеры отличаются по распределению признаков f01–f08, позволяют выделить группы с экстремальными значениями и средней вариацией  
- DS2: кластеры соответствуют плотным областям в пространстве x1–x2, шумовые точки из `z_noise`  
- DS4: кластеры отражают сочетание категориальных типов и числовых профилей, что полезно для дальнейшей сегментации  

## 6. Conclusion

- На числовых датасетах KMeans стабилен и удобен  
- DBSCAN эффективен на шумных и неравномерных данных  
- AgglomerativeClustering полезна для смешанных и высокоразмерных датасетов  
- Масштабирование и имputation критически важны для корректной работы алгоритмов  
- Silhouette, Davies-Bouldin и Calinski-Harabasz дают полное представление о качестве кластеризации  
- Устойчивость кластеров проверять обязательно  
- PCA 2D помогает визуально оценить структуру данных  
- Соблюдение честного протокола preprocessing → reproducible результаты
